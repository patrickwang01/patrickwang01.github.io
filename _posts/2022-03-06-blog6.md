---
layout: post
title: Blog Post 6 - Fake News Classifier
---

In this blog post, I will walk through the process of developing a fake news classifier using the machine learning package, TensorFlow.


## Introuction 

To begin, before we do anything else, we should load in all of the required packages we will use later on. 

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

import re
import string
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
```

These TensorFlow packages will enable us to develop pipelines for our machine learning models!

The dataset that I will be using is shown below: 

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```

Let's read the url into a Pandas dataframe first:

```python
news = pd.read_csv(train_url)
```

We can take a look at a snippet of the dataframe:

```python
news.head()
```
  <div id="df-afac3b5a-e52c-4c9e-b721-f5cafe00249f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-afac3b5a-e52c-4c9e-b721-f5cafe00249f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-afac3b5a-e52c-4c9e-b721-f5cafe00249f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-afac3b5a-e52c-4c9e-b721-f5cafe00249f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>

Let's store the number of possible choices available for our models first. In this case, it should be 2 since the result is either 0 (real) or 1 (fake).

```python
fake = len(news["fake"].unique())
```

## 1. Establishing a Dataset

After we have loaded in the example dataset, we need to create a Tensor dataset to use in our TensorFlow models. 

To make the text in the Pandas dataframe more simple, we are going to remove the stopwords in them. Some examples of stopwords include: 'a', 'is', 'but', etc. 

```python
import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords
stopwords = stopwords.words('english')
```

Now, we write a function that accepts a dataframe and outputs the Tensor dataset created within. 

```python
def make_dataset(df):

  # remove stopwords
  news["title"] = news["title"].apply(
    lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))

  news["text"] = news["text"].apply(
    lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))

  # create dataset 
  dataset = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : news[["title"]], 
            "text" : news[["text"]]
        }, 
     
        {
            "fake" : news[["fake"]]
        }
    )
  )
  
  # batch dataset
  dataset.batch(100)

  return dataset
```
We remove the stopwords found in the titles and text of each article using a list comprehension and the `apply` function with a lambda. Afterwards, we create our TensorFlow dataset with two inputs: the article title and the article text. There is one output, which is the `fake` column of our dataframe that consists of 0s and 1s. At the end, we batch the dataset into rows of 100 to decrease our models' training runtime later on. 

<br> 

Now, if we run the original Pandas dataframe through our new function, it will create our new Tensor dataset `news_data`:

```python
news_data = make_dataset(news)
```

With our new dataset, we need to split it up into a training set and a validation set for our machine learning models. We'll split off 20% of the entire dataset to use as our validation set, which leaves the other 80% for the training set! To decrease our training runtime ever further, we will batch the training set and validation set again by rows of 20. 

```python
# split train/validation into 80/20
training_size = int(0.80 * len(news_data))
validation_size = int(0.20 * len(news_data))

train_set = news_data.take(training_size).batch(20)
validation_set = news_data.skip(training_size).take(validation_size).batch(20)
```
<br> 

Now that we have our split-up dataset, we should calculate the base rate of our model so that we have a general idea of the accuracy of the baseline machine learning model. 

We will use an iterator that iterates through our train set and examines the labels of the `fake` column of the original Pandas dataframe. If the label appears to be `0`, then it's real and if it's `1`, then it's fake. If we assume our base model to be one that always outputs "fake" as the result, then everytime our iterator sees a `1`, it will increment a counter. At the end, we will divide our counter by the length of the training set to see our model's accuracy.

```python
# calculate base rate of model 
iterator = train_set.unbatch().map(
  lambda x, news_data: news_data["fake"]).as_numpy_iterator()

number = 0
for num in iterator:
  if num == 1:
    number += 1

print(number / training_size)
```
We see that our baseline model has an accuracy of around 52.2%! So, if we made no changes to the model, we would be right 52% of the time, which isn't very good. 

```
    0.521911019544518
``` 

## 2. Creating the TensorFlow Models

Now that we have prepared most of the data, we are ready to begin designing our machine learning TensorFlow models!

The models that we're going to build will help answer the following question: 

***When detecting fake news, is it most effective to focus on only the title of the article, the full text of the article, or both?***

To begin, since we have two inputs, `title` and `text`, we will create a separate `keras` input for each one. 

```python
# create inputs for models 

# title is one dimension string type
title = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

# text is 1D string type
text = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```
By examining the code, we can see that both inputs are of string type since both are just text. Further, since there is only one title and text per entry, they both have a shape of 1 dimension. 

To make our data even more simple and efficient for our TensorFlow models to read and run on, we will perform some text preprocessing first. This will include standardizing the text in which the text will be "cleaned" in a way. Capitals and punctuations will be removed to make it simpler. The second component will include the process of vectorization in which the text will be converted into numerical vectors that is much easier for TensorFlow to read. 

If we run the code block below:

```python
vocab_limit = 2000

# remove all punctuations + lowercase all letters
def standardization(data):
    lower = tf.strings.lower(data)
    remove_punctuation = tf.strings.regex_replace(
      lower, '[%s]' % re.escape(string.punctuation),'')

    return remove_punctuation 

# create a vectorization layer that converts objects to vectors
vectorize_layer = TextVectorization(
    standardize = standardization,
    max_tokens = vocab_limit, 
    output_mode = 'int',
    output_sequence_length = 500) 

# apply vectorized layer onto title and text 
vectorize_layer.adapt(train_set.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train_set.map(lambda x, y: x["text"]))
```
We create a function called `standardization` that will first make every letter lowercased and then remove all of the punctuations after. We then use this function as an argument for the vectorization layer we are writing. Once we finish the layer, we need to adapt it to the training set we made previously on both the title and text columns. With this, we have finally finished all of the data preparations and can design our first model!

## Model 1: Article Title Only

For our first model, we will only use the article title as the input. We will run the model and check its performance later on. 

Since we are using multiple inputs, we will need to use the `Functional` API instead of the usual `Sequential` API. Let's create our layers for the "title" component! To begin, we will run our `title` input we created before through our vectorization layer. Afterwards, we run it through the most important layer -- the `Embedding` layer -- in 3 dimensions. This layer will enable us to make a pretty cool visualization figure later on! Afterwards, to reduce the risk of overfitting, we add some `Dropout` layers in the middle. At the end, we add our `Dense` layer to cap off the layers. 

```python
# functional API layers for 'title'
title_layer = vectorize_layer(title)
title_layer = layers.Embedding(vocab_limit, 3, name = "embedding")(title_layer)
title_layer = layers.Dropout(0.2)(title_layer)
title_layer = layers.GlobalAveragePooling1D()(title_layer)
title_layer = layers.Dropout(0.2)(title_layer)
title_layer = layers.Dense(32, activation = 'relu')(title_layer)
```

Since we need to specify our output in our model, we will run it through one last `Dense` layer using our `title` layer we just made. 

```python
# 1st model (title only)
output = layers.Dense(fake, name = "fake")(title_layer)
```

Now, with our layers, we put them into the machine learning model by specifying the input with our `title` input and the output as our `output` layer. 

```python
# 1st model (title only)
model1 = keras.Model(
    inputs = [title,],
    outputs = output
)
```
We can check a summary of all of the layers in our model!

```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout (Dropout)           (None, 500, 3)            0         
                                                                     
     global_average_pooling1d (G  (None, 3)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 3)                 0         
                                                                     
     dense (Dense)               (None, 32)                128       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 6,194
    Trainable params: 6,194
    Non-trainable params: 0
    _________________________________________________________________
    

To compile and prepare our model, we run the code block below:

```python
model1.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
```
Now, we are officially ready to run our first model on the training and validation set we made in the beginning!

```python
history = model1.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```

To check the performance, we should write a function that plots the training and validation accuracy for every epoch in our model. 

If we run the code block below:

```python
# plot training vs validation history for model
def training_history(History):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(title = "Training vs Validation Accuracy", xlabel = "Epoch", 
                ylabel = "Accuracy")
  plt.legend()
```
Our output should be a graph consisting of two lines: the training set accuracy and the validation set accuracy with corresponding labels and colors. 

```python
# model 1 (title only)
training_history(history)
```

When we run our results through this function, we get the graph below and can check our first model's performance. 
    
![model1.png](/images/model1.png)
    
This is really good for our first model! As the epochs increase, our validation accuracy and training accuracy increase. Surprisingly, the validation accuracy is actually larger than the training accuracy at all epochs, which proves that there was no sign of overfitting! The `Dropout` layers were really effective then. Looking at the general overall trend, it looks like our first model hovers around 97% or 98% for our validation set? 

## Model 2: Article Text Only

Since our first model only used the article title as the input, our second model will use the article text as the only input. Similar to the first model, we will first establish the layers for the `text` input. 

<br>

Since our first model worked very well with the layers we made, we could use the same layers and not adjust too much and should probably get strong results still. To begin, we run our `text` input through the vectorization layer and then through the `Embedding` layer. Afterwards, we run it through a few `Dropout` layers to reduce overfitting and lastly, through a `Dense` layer to cap it off. 

```python
#functional API layers for 'text'
text_layer = vectorize_layer(text)
text_layer = layers.Embedding(vocab_limit, 3, name = "embedding")(text_layer)
text_layer = layers.Dropout(0.2)(text_layer)
text_layer = layers.GlobalAveragePooling1D()(text_layer)
text_layer = layers.Dropout(0.2)(text_layer)
text_layer = layers.Dense(32, activation = 'relu')(text_layer)
```

If we run the code block below, we will create our second TensorFlow model using the `text` layers and the `output` layers as the input and output respectively! 

```python
# 2nd model (text only)

output = layers.Dense(fake, name = "fake")(text_layer)

model2 = keras.Model(
    inputs = [text,],
    outputs = output
)

model2.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
```
We can check the model's summary below:

```python
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_1   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 3)                 0         
                                                                     
     dense_1 (Dense)             (None, 32)                128       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 6,194
    Trainable params: 6,194
    Non-trainable params: 0
    _________________________________________________________________
    

Let's run our second model using 20 epochs again similar to the first!
```python
history = model2.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```

Running our results through the plotting function, we can see the graph below:
```python
# model 2 (text only)
training_history(history)
```
    
![model2.png](/images/model2.png)
    
Our second model performs really well in the validation set, which hovers around 98% in the later epochs. This is really similar to the first model in terms of performance, however, there are signs of overfitting. As we can see, the training accuracy is higher than the validation accuracy after around 3 epochs, which suggests that our model is performing better and perhaps, too good on the training set. Overall, our second model using the article text as the only input is strong, but shows some evidence of overfitting. 

## Model 3: Both Article Title and Text

For our third and last model, we will use both the article title and the article text as the inputs.

For our third model, we will use the `text` layers that we originally created in our second model and the `title` layers, but with a few adjustments. For the `title` layers, we will remove the `Dropout` layers and the `Embedding` layer so that when we combine them later, the model will run smoothly. 

```python
# title component for model 3 (title + text)

title_layer = vectorize_layer(title)
title_layer = layers.Dense(32, activation = 'relu')(title_layer)
```

To combine the two layers, we use the `concatenate` attribute:

```python
# both inputs combined
combined = layers.concatenate([title_layer, text_layer], axis = 1)
```

And then we run the combined layer through a `Dense` layer and run the `output` layer through a `Dense` layer.

```python
combined = layers.Dense(32, activation='relu')(combined)
output = layers.Dense(fake, name = "fake")(combined)
```

Now, we can use input these layers into our model and compile it to prepare it for training!
```python
# model 3 (title + text)
model3 = keras.Model(
    inputs = [title, text],
    outputs = output
)

model3.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
```

```python
model3.summary()
```

    Model: "model_3"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['text[0][0]',                   
     ization)                                                         'title[0][0]']                  
                                                                                                      
     embedding (Embedding)          (None, 500, 3)       6000        ['text_vectorization[1][0]']     
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 3)       0           ['embedding[0][0]']              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 3)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_3 (Dropout)            (None, 3)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dense_2 (Dense)                (None, 32)           16032       ['text_vectorization[2][0]']     
                                                                                                      
     dense_1 (Dense)                (None, 32)           128         ['dropout_3[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense_2[0][0]',                
                                                                      'dense_1[0][0]']                
                                                                                                      
     dense_3 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_3[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 24,306
    Trainable params: 24,306
    Non-trainable params: 0
    __________________________________________________________________________________________________
    
As we can see from the model summary, we can see the coresponding layers for our `title` and `text` components and the `concatenation` portion near the end.

Now, we run our third and final model and pass it through our plotting function!

```python
history = model3.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```


```python
# model 3
training_history(history)
```
    
![model3.png](/images/model3.png)
    
As we can see from this training history, it also seems pretty strong in which the validation accuracy hovers a little over 98% overall, which is better than our other two models! However, once again, we see that there are signs of overfitting as shown by the higher training accuracy over most of the later epochs. Nonetheless, our third model, which uses both the article title and text as inputs, is the strongest one in terms of validation accuracy and we can use this model to evaluate our test set!

## Recommendation

After creating our three models, we found that the third one -- which uses article title and text -- is the strongest and performed the best on the validation set of our Tensor dataset. We can use this conclusion to make a recommendation on whether algorithms should use the title, text, or both to detect fake news. In our case, we recommend that algorithms should use both the title and text to detect fake news!


## 3. Model Evaluation

Now that we have fully trained our model on the training and validation set, we are ready to evaluate its performance on unseen data. 

We can load in the testing set by running the code block below:

```python
# load in test set 
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_set = pd.read_csv(test_url)
```

Running the testing set dataframe through the `make_dataset` function will create a Tensor dataset and we can run our model on this dataset.

```python
# make test set into tf dataset
test_news = make_dataset(test_set)
```

If we run the block below:

```python
# test score using model 3
model3.evaluate(test_news)
```

    22449/22449 [==============================] - 44s 2ms/step - loss: 0.0211 - accuracy: 0.9961

    [0.021113725379109383, 0.9961245656013489]

We can see that our accuracy on the testing set is 99.6%, which is extremely good, almost perfect! This is a sure sign that our model which uses the title and text, is really strong at predicting unseen data. If we were to use our model in predicting fake news, we would accurately predict fake news 99.6% of the time, which is pretty much every fake article!


## 4. Embedded Visualization

Now that we have determined our model's performance on predicting unseen data, we could transition to visualizing how much a word corresponds to "fake" or "real" articles! 

Since our model uses the `Embedding` layer, it's possible to obtain the weights from that layer to use for our visualization. We will then get all of the words found in the dataset using the `vectorize` layer from before. 

If we run the code block below:

```python
# embedding visualizations (model 3)
weight = model3.get_layer('embedding').get_weights()[0] # get weights
word_list = vectorize_layer.get_vocabulary()   # get all words found in dataset     

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)  # 2D PCA transformation
weights = pca.fit_transform(weight)

# create embedded dataframe with weights
embedding_df = pd.DataFrame({
    'word' : word_list, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```
Using the principal component analysis (PCA) to reduce our three-dimensional weights into two, we can create a data frame that contains all of the weight proportions for each word in the data set. We can then plot this data frame into a `plotly` scatter plot to see our embedded visualization figure!

```python
import plotly.express as px 

# plot embedded dataframe to visualize
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size_max = 2,
                 hover_name = "word",
                 title = "Embedded Visualization for Fake News Classification")

fig.show()
```

![embedded-visual.png](/images/embedded-visual.png)

As we can see from the figure, each dot represents a word and their weight proportions. The larger the magnitude in either direction, the more times it corresponds to "fake" articles. For instance, the word `trumps` appears on the farthest right spectrum and thus, has a large magnitude in the horizontal axis. This suggests that that word appears more in fake news articles. It does seem more related to Trump and it is a strong word with a somewhat negative connotation. 

Another word that we could interpret is `voters`, which appears extremely close to the (0,0) coordinate. This suggests that this word is more associated with real news article and this makes sense since voters is one of the main components in politics and most political news is written by reputable companies. 

Another word that we could interpret is `gop`, which appears on the farthest left spectrum on the figure. This suggests that it's more associated with fake news. The GOP represents the Republican party and a lot of fake news is usually pretty conservative, so this makes sense. 

Another word we could intepret is `cyber`, which appears near the (0,0) coordinate. This suggests that it is more associated with real news and this makes sense because news about technology is usually true. It's mostly related to technological progress, which doesn't really appear in fake news.

Lastly, a fifth word that we could interpret is `leading`, which appears farthest down from (0,0). Usually, "leading" appears in politically related news about elections and other relevant topics. Fake news is more prominent in politics, so it makes sense that "leading" is more associated with fake news.

As we can see from these five words, it seems that words more closely related to politics are usually more related to fake news, which logically makes sense since bias and strong opinions are prominent in this field. 

## Conclusion

Using TensorFlow, we were able to create a machine learning model that accurately predicts whether a news article is fake or not. The `Functional` API enables us to implement multiple inputs into a model and the `Embedding` layer allows for cool visualization figures! 