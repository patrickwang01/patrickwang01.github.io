---
layout: post
title: Blog Post 6
---

```python
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers.experimental import preprocessing
from tensorflow.keras import datasets, layers, models
from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
from tensorflow.keras.layers.experimental.preprocessing import StringLookup

import re
import string
import numpy as np
from matplotlib import pyplot as plt
import pandas as pd
```
\

```python
train_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_train.csv?raw=true"
```


```python
news = pd.read_csv(train_url)
```


```python
news.head()
```





  <div id="df-afac3b5a-e52c-4c9e-b721-f5cafe00249f">
    <div class="colab-df-container">
      <div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>title</th>
      <th>text</th>
      <th>fake</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>17366</td>
      <td>Merkel: Strong result for Austria's FPO 'big c...</td>
      <td>German Chancellor Angela Merkel said on Monday...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5634</td>
      <td>Trump says Pence will lead voter fraud panel</td>
      <td>WEST PALM BEACH, Fla.President Donald Trump sa...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>17487</td>
      <td>JUST IN: SUSPECTED LEAKER and â€œClose Confidant...</td>
      <td>On December 5, 2017, Circa s Sara Carter warne...</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>12217</td>
      <td>Thyssenkrupp has offered help to Argentina ove...</td>
      <td>Germany s Thyssenkrupp, has offered assistance...</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5535</td>
      <td>Trump say appeals court decision on travel ban...</td>
      <td>President Donald Trump on Thursday called the ...</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
      <button class="colab-df-convert" onclick="convertToInteractive('df-afac3b5a-e52c-4c9e-b721-f5cafe00249f')"
              title="Convert this dataframe to an interactive table."
              style="display:none;">

  <svg xmlns="http://www.w3.org/2000/svg" height="24px"viewBox="0 0 24 24"
       width="24px">
    <path d="M0 0h24v24H0V0z" fill="none"/>
    <path d="M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z"/><path d="M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z"/>
  </svg>
      </button>

  <style>
    .colab-df-container {
      display:flex;
      flex-wrap:wrap;
      gap: 12px;
    }

    .colab-df-convert {
      background-color: #E8F0FE;
      border: none;
      border-radius: 50%;
      cursor: pointer;
      display: none;
      fill: #1967D2;
      height: 32px;
      padding: 0 0 0 0;
      width: 32px;
    }

    .colab-df-convert:hover {
      background-color: #E2EBFA;
      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);
      fill: #174EA6;
    }

    [theme=dark] .colab-df-convert {
      background-color: #3B4455;
      fill: #D2E3FC;
    }

    [theme=dark] .colab-df-convert:hover {
      background-color: #434B5C;
      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);
      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));
      fill: #FFFFFF;
    }
  </style>

      <script>
        const buttonEl =
          document.querySelector('#df-afac3b5a-e52c-4c9e-b721-f5cafe00249f button.colab-df-convert');
        buttonEl.style.display =
          google.colab.kernel.accessAllowed ? 'block' : 'none';

        async function convertToInteractive(key) {
          const element = document.querySelector('#df-afac3b5a-e52c-4c9e-b721-f5cafe00249f');
          const dataTable =
            await google.colab.kernel.invokeFunction('convertToInteractive',
                                                     [key], {});
          if (!dataTable) return;

          const docLinkHtml = 'Like what you see? Visit the ' +
            '<a target="_blank" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'
            + ' to learn more about interactive tables.';
          element.innerHTML = '';
          dataTable['output_type'] = 'display_data';
          await google.colab.output.renderOutput(dataTable, element);
          const docLink = document.createElement('div');
          docLink.innerHTML = docLinkHtml;
          element.appendChild(docLink);
        }
      </script>
    </div>
  </div>





```python
fake = len(news["fake"].unique())
```


```python
import nltk
nltk.download('stopwords')
```

```python
from nltk.corpus import stopwords
stopwords = stopwords.words('english')


news["title"] = news["title"].apply(lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))
news["text"] = news["text"].apply(lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))
```


```python
dataset = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : news[["title"]], 
            "text" : news[["text"]]
        }, 
     
        {
            "fake" : news[["fake"]]
        }
    )
)
```


```python
def make_dataset(df):

  # remove stopwords
  news["title"] = news["title"].apply(lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))
  news["text"] = news["text"].apply(lambda x: ' '.join([i for i in x.split() if i not in (stopwords)]))

  # create dataset 
  dataset = tf.data.Dataset.from_tensor_slices(
    (
        {
            "title" : news[["title"]], 
            "text" : news[["text"]]
        }, 
     
        {
            "fake" : news[["fake"]]
        }
    )
  )
  
  # batch dataset
  dataset.batch(100)

  return dataset
  
```


```python
news_data = make_dataset(news)
```


```python
# split train/validation into 80/20
training_size = int(0.80 * len(news_data))
validation_size = int(0.20 * len(news_data))

train_set = news_data.take(training_size).batch(20)
validation_set = news_data.skip(training_size).take(validation_size).batch(20)
```


```python
# calculate base rate of model 
iterator = train_set.unbatch().map(lambda x, news_data: news_data["fake"]).as_numpy_iterator()

number = 0
for num in iterator:
  if num == 1:
    number += 1

print(number / training_size)
```

    0.521911019544518
    


```python
# create inputs for models 

# title is one dimension string type
title = keras.Input(
    shape = (1,), 
    name = "title",
    dtype = "string"
)

# text is 1D string type
text = keras.Input(
    shape = (1,), 
    name = "text",
    dtype = "string"
)
```


```python
vocab_limit = 2000

# remove all punctuations + lowercase all letters
def standardization(data):
    lower = tf.strings.lower(data)
    remove_punctuation = tf.strings.regex_replace(lower, '[%s]' % re.escape(string.punctuation),'')

    return remove_punctuation 

vectorize_layer = TextVectorization(
    standardize = standardization,
    max_tokens = vocab_limit, # only consider this many words
    output_mode = 'int',
    output_sequence_length = 500) 

vectorize_layer.adapt(train_set.map(lambda x, y: x["title"]))
vectorize_layer.adapt(train_set.map(lambda x, y: x["text"]))
```


```python
# functional API layers for 'title'
title_layer = vectorize_layer(title)
title_layer = layers.Embedding(vocab_limit, 3, name = "embedding")(title_layer)
title_layer = layers.Dropout(0.2)(title_layer)
title_layer = layers.GlobalAveragePooling1D()(title_layer)
title_layer = layers.Dropout(0.2)(title_layer)
title_layer = layers.Dense(32, activation = 'relu')(title_layer)
```


```python
#functional API layers for 'text'
text_layer = vectorize_layer(text)
text_layer = layers.Embedding(vocab_limit, 3, name = "embedding")(text_layer)
text_layer = layers.Dropout(0.2)(text_layer)
text_layer = layers.GlobalAveragePooling1D()(text_layer)
text_layer = layers.Dropout(0.2)(text_layer)
text_layer = layers.Dense(32, activation = 'relu')(text_layer)
```


```python
# 1st model (title only)
output = layers.Dense(fake, name = "fake")(title_layer)

```


```python
# 1st model (title only)
model1 = keras.Model(
    inputs = [title,],
    outputs = output
)

```


```python
model1.summary()
```

    Model: "model"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     title (InputLayer)          [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout (Dropout)           (None, 500, 3)            0         
                                                                     
     global_average_pooling1d (G  (None, 3)                0         
     lobalAveragePooling1D)                                          
                                                                     
     dropout_1 (Dropout)         (None, 3)                 0         
                                                                     
     dense (Dense)               (None, 32)                128       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 6,194
    Trainable params: 6,194
    Non-trainable params: 0
    _________________________________________________________________
    


```python
model1.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
```


```python
history = model1.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['text'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    


```python
# plot training vs validation history for model
def training_history(History):
  plt.plot(history.history["accuracy"], label = "training")
  plt.plot(history.history["val_accuracy"], label = "validation")
  plt.gca().set(title = "Training vs Validation Accuracy", xlabel = "Epoch", ylabel = "Accuracy")
  plt.legend()
```


```python
# model 1
training_history(history)
```


    
![model1.png](/images/model1.png)
    



```python
# 2nd model (text only)

output = layers.Dense(fake, name = "fake")(text_layer)

model2 = keras.Model(
    inputs = [text,],
    outputs = output
)

model2.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)

```


```python
model2.summary()
```

    Model: "model_1"
    _________________________________________________________________
     Layer (type)                Output Shape              Param #   
    =================================================================
     text (InputLayer)           [(None, 1)]               0         
                                                                     
     text_vectorization (TextVec  (None, 500)              0         
     torization)                                                     
                                                                     
     embedding (Embedding)       (None, 500, 3)            6000      
                                                                     
     dropout_2 (Dropout)         (None, 500, 3)            0         
                                                                     
     global_average_pooling1d_1   (None, 3)                0         
     (GlobalAveragePooling1D)                                        
                                                                     
     dropout_3 (Dropout)         (None, 3)                 0         
                                                                     
     dense_1 (Dense)             (None, 32)                128       
                                                                     
     fake (Dense)                (None, 2)                 66        
                                                                     
    =================================================================
    Total params: 6,194
    Trainable params: 6,194
    Non-trainable params: 0
    _________________________________________________________________
    


```python
history = model2.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```

    /usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:559: UserWarning: Input dict contained keys ['title'] which did not match any model input. They will be ignored by the model.
      inputs = self._flatten_to_reference_inputs(inputs)
    


```python
# model 2
training_history(history)
```


    
![model2.png](/images/model2.png)
    



```python
# title component for model 3 (title + text)

title_layer = vectorize_layer(title)
#title_layer = layers.Dropout(0.2)(title_layer)
#title_layer = layers.GlobalAveragePooling1D()(title_layer)
#title_layer = layers.Dropout(0.2)(title_layer)
title_layer = layers.Dense(32, activation = 'relu')(title_layer)
```


```python
# both inputs combined
combined = layers.concatenate([title_layer, text_layer], axis = 1)
```


```python
combined = layers.Dense(32, activation='relu')(combined)
output = layers.Dense(fake, name = "fake")(combined)
```


```python
# model 3 (title + text)
model3 = keras.Model(
    inputs = [title, text],
    outputs = output
)

model3.compile(optimizer = "adam",
              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),
              metrics = ['accuracy']
)
```


```python
model3.summary()
```

    Model: "model_3"
    __________________________________________________________________________________________________
     Layer (type)                   Output Shape         Param #     Connected to                     
    ==================================================================================================
     title (InputLayer)             [(None, 1)]          0           []                               
                                                                                                      
     text (InputLayer)              [(None, 1)]          0           []                               
                                                                                                      
     text_vectorization (TextVector  (None, 500)         0           ['text[0][0]',                   
     ization)                                                         'title[0][0]']                  
                                                                                                      
     embedding (Embedding)          (None, 500, 3)       6000        ['text_vectorization[1][0]']     
                                                                                                      
     dropout_2 (Dropout)            (None, 500, 3)       0           ['embedding[0][0]']              
                                                                                                      
     global_average_pooling1d_1 (Gl  (None, 3)           0           ['dropout_2[0][0]']              
     obalAveragePooling1D)                                                                            
                                                                                                      
     dropout_3 (Dropout)            (None, 3)            0           ['global_average_pooling1d_1[0][0
                                                                     ]']                              
                                                                                                      
     dense_2 (Dense)                (None, 32)           16032       ['text_vectorization[2][0]']     
                                                                                                      
     dense_1 (Dense)                (None, 32)           128         ['dropout_3[0][0]']              
                                                                                                      
     concatenate (Concatenate)      (None, 64)           0           ['dense_2[0][0]',                
                                                                      'dense_1[0][0]']                
                                                                                                      
     dense_3 (Dense)                (None, 32)           2080        ['concatenate[0][0]']            
                                                                                                      
     fake (Dense)                   (None, 2)            66          ['dense_3[0][0]']                
                                                                                                      
    ==================================================================================================
    Total params: 24,306
    Trainable params: 24,306
    Non-trainable params: 0
    __________________________________________________________________________________________________
    


```python
history = model3.fit(train_set, 
                    validation_data = validation_set,
                    epochs = 20, 
                    verbose = False)
```


```python
# model 3
training_history(history)
```


    
![model3.png](/images/model3.png)
    



```python
# load in test set 
test_url = "https://github.com/PhilChodrow/PIC16b/blob/master/datasets/fake_news_test.csv?raw=true"

test_set = pd.read_csv(test_url)
```


```python
# make test set into tf dataset
test_news = make_dataset(test_set)
```


```python
# test score using model 3
model3.evaluate(test_news)
```

    22449/22449 [==============================] - 44s 2ms/step - loss: 0.0211 - accuracy: 0.9961
    




    [0.021113725379109383, 0.9961245656013489]




```python
# embedding visualizations (model 3)
weight = model3.get_layer('embedding').get_weights()[0] # get the weights from the embedding layer
word_list = vectorize_layer.get_vocabulary()   # get all words found in dataset     

from sklearn.decomposition import PCA
pca = PCA(n_components = 2)  # 2D PCA
weights = pca.fit_transform(weight)

# create embedded dataframe with weights
embedding_df = pd.DataFrame({
    'word' : word_list, 
    'x0'   : weights[:,0],
    'x1'   : weights[:,1]
})
```


```python
import plotly.express as px 

# plot embedded dataframe to visualize
fig = px.scatter(embedding_df, 
                 x = "x0", 
                 y = "x1", 
                 size_max = 2,
                 hover_name = "word",
                 title = "Embedded Visualization for Fake News Classification")

fig.show()
```